"""
Challenge Nexialog MoSEF
Interface Streamlit :
- Permet de simuler une nouvelle heure en ins√©rant des anomalies dans des noeuds
  choisis (Boucles, PEAG, OLT). 
- D√©tecte les noeuds anormaux et les affiche sous formes
  de tableau. 
- Visualisation 3D pour voir les anomalies avec approche unidimensionnelle
ou multidimensionnelle (Isolation Forest)
"""

import streamlit as st
import numpy as np
import pandas as pd
from scipy.stats import gaussian_kde
import joblib
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from tqdm import tqdm
import plotly.express as px

from utils import import_json_to_dict
from nodes_checker import NodesChecker
from anomaly_detection_isolation_forest import MultiIsolationForestDetector
from anomaly_detection_mahalanobis import MahalanobisDetector


# Configuration 
st.set_page_config(
    page_title="Challenge Nexialog - D√©tection d'anomalies",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded",
)

def initialize_session_state():
    if "donnees_chargees" not in st.session_state:
        # Importation de l'heure simul√©e
        st.session_state.lignes_1fev = pd.read_csv('data/results/lignes_1fev.csv', index_col=0).replace([np.inf, -np.inf], np.nan).dropna().head(600)
        st.session_state.lignes_1fev_copy = st.session_state.lignes_1fev.copy()
        # Importation des vecteurs de distribution par test
        st.session_state.dict_distribution_test = import_json_to_dict("data/results/dict_test.json")
        # Colonnes dans lesquelles injecter et rep√©rer des anomalies
        st.session_state.variables_test = [
            'avg_score_scoring',
            'avg_latence_scoring',
            'avg_dns_time',
            'std_score_scoring',
            'std_latence_scoring',
            'std_dns_time'
        ]
        
        st.session_state.df_with_anomalies = None

        # Nouvelles colonnes de p_values √† ajouter
        st.session_state.p_values_col = [
            'p_val_avg_dns_time',
            'p_val_avg_score_scoring',
            'p_val_avg_latence_scoring',
            'p_val_std_dns_time',
            'p_val_std_score_scoring',
            'p_val_std_latence_scoring'
        ]

        st.session_state.p_value_threshold = 5.0  # Seuil de sensibilit√©/rejet
        st.session_state.donnees_chargees = True
        
        # Initialisation du d√©tecteur Isolation Forest
        st.session_state.isolation_forest =  MultiIsolationForestDetector(chain_id_col = 'name')
         # chargement des modeles d'isolation forest
        st.session_state.isolation_forest.load_models(st.session_state.lignes_1fev_copy)

        st.session_state.isolation_forest_threshold = 0.00
        
        # Variables pour stocker les r√©sultats des analyses
        st.session_state.results = {
            "boucles": None,
            "peag_nro": None,
            "olt": None
        }
       
        st.session_state.anomalies_detected = False

# Fonction pour afficher l'en-t√™te de l'application
def show_header():
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col1:
        st.image("images/SFR_logo.png", width=120, use_container_width=False)
    
    with col2:
        st.title("D√©tection d'anomalies sur le r√©seau")
        st.caption("Challenge Nexialog & Universit√© Paris 1 Panth√©on-Sorbonne")
    
    with col3:
        st.image("images/nexialog_logo.png", width=120, use_container_width=False)
    
    # CSS pour l'alignement
    st.markdown("""
    <style>
    [data-testid="stHorizontalBlock"] {
        align-items: center;
    }
    [data-testid="stHorizontalBlock"] > div:first-child {
        display: flex;
        justify-content: flex-start;
        align-items: center;
        padding-left: 10px;
    }
    [data-testid="stHorizontalBlock"] > div:last-child {
        display: flex;
        justify-content: flex-end;
        align-items: center;
        padding-right: 10px;
    }
    [data-testid="stHorizontalBlock"] > div:nth-child(2) {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown("---")



def show_network_status():
    st.header("üìä √âtat du r√©seau")
    
    # Section 1: Statistiques globales du r√©seau
    st.subheader("Statistiques globales")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        # Nombre d'√©quipements actifs
        olt_count = len(st.session_state.lignes_1fev['olt_name'].unique())
        peag_count = len(st.session_state.lignes_1fev['peag_nro'].unique())
        boucle_count = len(st.session_state.lignes_1fev['boucle'].unique())
        total_equipment = olt_count + peag_count + boucle_count
        
        st.metric(label="√âquipements actifs", 
                 value=f"{total_equipment}",
                 help="Somme des OLT, PEAG et Boucles actifs")
    
    with col2:
        # Couverture du r√©seau
        dept_count = len(st.session_state.lignes_1fev['code_departement'].unique())
        st.metric(label="D√©partements couverts", 
                  value=f"{dept_count}")
    
    with col3:
        # Volume de tests
        dns_tests = st.session_state.lignes_1fev['nb_test_dns'].sum()
        scoring_tests = st.session_state.lignes_1fev['nb_test_scoring'].sum()
        total_tests = dns_tests + scoring_tests
        
        st.metric(label="Tests effectu√©s", 
                  value=f"{total_tests:,}".replace(',', ' '),
                  help="Somme des tests DNS et Scoring")
    
    # Section 2: Indicateurs de performance
    st.subheader("Indicateurs de performance r√©seau")
    
    col1, col2, col3 = st.columns(3)
    
    # Temps de r√©ponse moyen
    avg_dns = st.session_state.lignes_1fev['avg_dns_time'].mean()
    avg_latence = st.session_state.lignes_1fev['avg_latence_scoring'].mean()
    
    with col1:
        st.metric(label="Temps DNS moyen (ms)", 
                  value=f"{avg_dns:.2f}")
    
    with col2:
        st.metric(label="Latence scoring moyenne (ms)", 
                  value=f"{avg_latence:.2f}")
    
    with col3:
        # Score moyen du r√©seau
        avg_score = st.session_state.lignes_1fev['avg_score_scoring'].mean()
        max_score = 5.0  # Score maximum possible
        score_percentage = (avg_score / max_score) * 100
        
        st.metric(label="Score qualit√© r√©seau (%)", 
                  value=f"{score_percentage:.1f}")
    
    # Section 3: Calcul des indicateurs de sant√©
    st.subheader("Indicateurs de sant√© r√©seau")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        # Stabilit√© DNS
        std_dns = st.session_state.lignes_1fev['std_dns_time'].mean()
        dns_stability = 100 - min(100, (std_dns / avg_dns * 100))
        
        delta_color = "normal"
        if dns_stability < 80:
            delta_color = "off"
        elif dns_stability > 95:
            delta_color = "inverse"
            
        st.metric(label="Stabilit√© DNS (%)", 
                  value=f"{dns_stability:.1f}",
                  delta=f"{dns_stability-90:.1f}" if dns_stability != 90 else None,
                  delta_color=delta_color)
    
    with col2:
        # Stabilit√© latence
        std_latence = st.session_state.lignes_1fev['std_latence_scoring'].mean()
        latence_stability = 100 - min(100, (std_latence / avg_latence * 100))
        
        delta_color = "normal"
        if latence_stability < 80:
            delta_color = "off"
        elif latence_stability > 95:
            delta_color = "inverse"
            
        st.metric(label="Stabilit√© latence (%)", 
                  value=f"{latence_stability:.1f}",
                  delta=f"{latence_stability-90:.1f}" if latence_stability != 90 else None,
                  delta_color=delta_color)
    
    with col3:
        # Score global de sant√© r√©seau (moyenne pond√©r√©e)
        health_score = (dns_stability * 0.4 + latence_stability * 0.3 + score_percentage * 0.3)
        
        delta_color = "normal"
        if health_score < 80:
            delta_color = "off"
        elif health_score > 95:
            delta_color = "inverse"
            
        st.metric(label="Sant√© globale du r√©seau (%)", 
                  value=f"{health_score:.1f}",
                  delta=f"{health_score-90:.1f}" if health_score != 90 else None,
                  delta_color=delta_color)
     
    # Section 3: Statistiques des m√©triques
    st.subheader("Statistiques des m√©triques")
    
    metrics_df = pd.DataFrame({
        "M√©trique": [
            "Temps DNS moyen (ms)",
            "√âcart-type DNS (ms)",
            "Latence scoring moyenne (ms)",
            "√âcart-type latence (ms)",
            "Score qualit√© moyen (0-5)",
            "√âcart-type score"
        ],
        "Moyenne": [
            f"{st.session_state.lignes_1fev['avg_dns_time'].mean():.2f}",
            f"{st.session_state.lignes_1fev['std_dns_time'].mean():.2f}",
            f"{st.session_state.lignes_1fev['avg_latence_scoring'].mean():.2f}",
            f"{st.session_state.lignes_1fev['std_latence_scoring'].mean():.2f}",
            f"{st.session_state.lignes_1fev['avg_score_scoring'].mean():.2f}",
            f"{st.session_state.lignes_1fev['std_score_scoring'].mean():.2f}"
        ],
        "M√©diane": [
            f"{st.session_state.lignes_1fev['avg_dns_time'].median():.2f}",
            f"{st.session_state.lignes_1fev['std_dns_time'].median():.2f}",
            f"{st.session_state.lignes_1fev['avg_latence_scoring'].median():.2f}",
            f"{st.session_state.lignes_1fev['std_latence_scoring'].median():.2f}",
            f"{st.session_state.lignes_1fev['avg_score_scoring'].median():.2f}",
            f"{st.session_state.lignes_1fev['std_score_scoring'].median():.2f}"
        ],
        "Minimum": [
            f"{st.session_state.lignes_1fev['avg_dns_time'].min():.2f}",
            f"{st.session_state.lignes_1fev['std_dns_time'].min():.2f}",
            f"{st.session_state.lignes_1fev['avg_latence_scoring'].min():.2f}",
            f"{st.session_state.lignes_1fev['std_latence_scoring'].min():.2f}",
            f"{st.session_state.lignes_1fev['avg_score_scoring'].min():.2f}",
            f"{st.session_state.lignes_1fev['std_score_scoring'].min():.2f}"
        ],
        "Maximum": [
            f"{st.session_state.lignes_1fev['avg_dns_time'].max():.2f}",
            f"{st.session_state.lignes_1fev['std_dns_time'].max():.2f}",
            f"{st.session_state.lignes_1fev['avg_latence_scoring'].max():.2f}",
            f"{st.session_state.lignes_1fev['std_latence_scoring'].max():.2f}",
            f"{st.session_state.lignes_1fev['avg_score_scoring'].max():.2f}",
            f"{st.session_state.lignes_1fev['std_score_scoring'].max():.2f}"
        ]
    })
    
    st.dataframe(metrics_df, use_container_width=True, hide_index=True)
    
        # Section 4: Distributions des m√©triques principales
    st.subheader("Distribution des m√©triques principales")
    
    # Cr√©er un tableau pour les histogrammes
    metric_mapping = {
        'avg_dns_time': 'Temps DNS moyen',
        'avg_latence_scoring': 'Latence moyenne',
        'avg_score_scoring': 'Score qualit√©'
    }
    
    selected_metric = st.selectbox(
        "Choisir une m√©trique",
        options=list(metric_mapping.keys()),
        format_func=lambda x: metric_mapping[x]
    )
    
    # Cr√©er un histogramme pour la m√©trique s√©lectionn√©e
    fig = px.histogram(
        st.session_state.lignes_1fev,
        x=selected_metric,
        nbins=50,
        title=f"Distribution de {metric_mapping[selected_metric]}",
        labels={selected_metric: metric_mapping[selected_metric]}
    )
    
    # Ajouter une ligne verticale pour la moyenne
    mean_value = st.session_state.lignes_1fev[selected_metric].mean()
    fig.add_vline(x=mean_value, line_dash="dash", line_color="red", annotation_text=f"Moyenne: {mean_value:.2f}")
    
    st.plotly_chart(fig, use_container_width=True)



def display_node_anomaly_chart():
    """
    Cr√©e un graphique de synth√®se montrant la distribution des anomalies par type de n≈ìud
    et leur r√©partition par variable.
    """
    if not st.session_state.anomalies_detected:
        return
    
    # Cr√©er deux colonnes pour les graphiques
    col1, col2 = st.columns(2)
    
    with col1:
        # Cr√©er un diagramme √† barres pour comparer les proportions d'anomalies par type de n≈ìud
        node_types = ['PEAG', 'OLT']
        anomaly_percentages = []

        if st.session_state.results["peag_nro"] is not None:
            df = st.session_state.results["peag_nro"]
            p_val_min = df[['p_val_avg_dns_time', 'p_val_avg_score_scoring', 'p_val_avg_latence_scoring']].min(axis=1)
            condition = (p_val_min < st.session_state.p_value_threshold / 100) | (df['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold)
            anomaly_percentages.append(round(100 * sum(condition) / len(df), 2))
        else:
            anomaly_percentages.append(0)
        
        if st.session_state.results["olt"] is not None:
            df = st.session_state.results["olt"]
            p_val_min = df[['p_val_avg_dns_time', 'p_val_avg_score_scoring', 'p_val_avg_latence_scoring']].min(axis=1)
            condition = (p_val_min < st.session_state.p_value_threshold / 100) | (df['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold)
            anomaly_percentages.append(round(100 * sum(condition) / len(df), 2))
        else:
            anomaly_percentages.append(0)
        
        fig = go.Figure([
            go.Bar(
                x=node_types,
                y=anomaly_percentages,
                marker_color=['#66B3FF', '#99FF99'],  # Seulement deux couleurs maintenant
                text=anomaly_percentages,
                textposition='auto',
            )
        ])
        
        fig.update_layout(
            title="Pourcentage de n≈ìuds anormaux par type (IsolationForest + Unidimensionnel)",
            xaxis_title="Type de n≈ìud",
            yaxis_title="Pourcentage d'anomalies (%)",
            yaxis=dict(range=[0, 100]),
            height=350,
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Remplacer le graphique radar par un graphique √† barres horizontales
        metrics = ['avg_dns_time', 'std_dns_time', 'avg_score_scoring', 
                  'std_score_scoring', 'avg_latence_scoring', 'std_latence_scoring']
        
        # Noms plus lisibles pour l'affichage
        display_names = [
            'Temps DNS moyen', 
            '√âcart-type DNS', 
            'Score scoring moyen', 
            '√âcart-type scoring', 
            'Latence scoring moyenne', 
            '√âcart-type latence'
        ]
        
        # Compter le nombre d'anomalies par m√©trique (toutes variables)
        anomaly_counts = [0] * len(metrics)
        threshold_p_val = st.session_state.p_value_threshold / 100
        
        for i, metric in enumerate(metrics):
            p_val_col = f'p_val_{metric}'
            
            for node_type in ["boucles", "peag_nro", "olt"]:
                if st.session_state.results[node_type] is not None and p_val_col in st.session_state.results[node_type].columns:
                    anomaly_counts[i] += (st.session_state.results[node_type][p_val_col] < threshold_p_val).sum()
        
        # Cr√©er le graphique √† barres horizontales
        colors = ['#FF9999', '#66B3FF', '#99FF99', '#FFCC99', '#C2C2F0', '#FFB3E6']
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            y=display_names,
            x=anomaly_counts,
            orientation='h',
            marker_color=colors,
            text=anomaly_counts,
            textposition='auto',
        ))
        
        fig.update_layout(
            title="Nombre d'anomalies par m√©trique (d√©tection unidimensionnelle)",
            xaxis_title="Nombre d'anomalies",
            yaxis=dict(
                categoryorder='total ascending',  # Tri par nombre d'anomalies
            ),
            height=350,
        )
        
        st.plotly_chart(fig, use_container_width=True)

# Fonction pour afficher le r√©sum√© des anomalies (approche unidimensionnelle)
def display_anomaly_summary():
    results = st.session_state.results
    # D√©finir les seuils
    threshold_p_val = st.session_state.p_value_threshold / 100
    threshold_if = st.session_state.isolation_forest_threshold
    
    # Cr√©er trois colonnes principales
    st.subheader("R√©sum√© des anomalies d√©tect√©es (Isolation Forest + Unidimensionnel)")
    col1, col2 = st.columns(2)
    
    # Fonction pour cr√©er une jauge d'anomalies
    def create_gauge_chart(percentage, title):
            fig = go.Figure(go.Indicator(
                mode="gauge+number",
                value=percentage,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': title},
                gauge={
                    'axis': {
                        'range': [0, 100],
                        'tickwidth': 1,
                        'tickcolor': "darkblue",
                        'tickvals': [0, 25, 50, 75, 100],
                        'ticktext': ['0', '25', '50', '75', '100'],
                        'tickangle': 0,  # Assurez-vous que le texte est horizontal
                        'tickfont': {'size': 12}  # R√©duire la taille des √©tiquettes
                    },
                    'bar': {'color': "darkblue"},
                    'bgcolor': "white",
                    'borderwidth': 2,
                    'bordercolor': "gray",
                    'steps': [
                        {'range': [0, 25], 'color': 'green'},
                        {'range': [25, 50], 'color': 'yellow'},
                        {'range': [50, 75], 'color': 'orange'},
                        {'range': [75, 100], 'color': 'red'}
                    ],
                }
            ))
            
            # Augmenter les marges √† droite pour √©viter les coupures
            fig.update_layout(
                height=200,
                width=320,  # D√©finir une largeur fixe
                margin=dict(l=20, r=50, t=50, b=20),  # Augmenter la marge droite
                paper_bgcolor="rgba(0,0,0,0)",  # Fond transparent
                plot_bgcolor="rgba(0,0,0,0)"
            )
            return fig
    
    with col1:
        if results["peag_nro"] is not None:
            
            df = results["peag_nro"]
            p_val_min = df[['p_val_avg_dns_time', 'p_val_avg_score_scoring', 'p_val_avg_latence_scoring']].min(axis=1)
            condition = (p_val_min < threshold_p_val) | (df['avg_isolation_forest_score'] < threshold_if)
            anomalous_peag = df[condition]
            
            # Pourcentage de PEAG anormaux
            peag_percent = round(100 * len(anomalous_peag) / len(results["peag_nro"]), 2)
            
            # Afficher la jauge
            st.plotly_chart(create_gauge_chart(peag_percent, f"% de PEAG anormaux"), use_container_width=True)
            
            st.metric("Nbr PEAG anormaux / Total", f"{len(anomalous_peag)} / {len(results['peag_nro'])}")

            if not anomalous_peag.empty:
                # Trouver les PEAG les plus probl√©matiques
                worst_peag = anomalous_peag.sort_values('avg_isolation_forest_score').index[:3]
                
                # Cr√©er une table avec formatage color√© pour les PEAG les plus anormaux
                st.markdown("#### PEAG les plus anormaux:")
                for idx in worst_peag:
                    score = anomalous_peag.loc[idx, 'avg_isolation_forest_score']
                    color = 'red' if score < threshold_if else 'orange'
                    st.markdown(f"<span style='color:{color};font-weight:bold;font-size:16px'>‚Ä¢ {idx}</span> (score: {score:.3f})", unsafe_allow_html=True)
    
    with col2:
        if results["olt"] is not None:
            df = results["olt"]
            p_val_min = df[['p_val_avg_dns_time', 'p_val_avg_score_scoring', 'p_val_avg_latence_scoring']].min(axis=1)
            condition = (p_val_min < threshold_p_val) | (df['avg_isolation_forest_score'] < threshold_if)
            anomalous_olt = df[condition]
            
            # Pourcentage d'OLT anormaux
            olt_percent = round(100 * len(anomalous_olt) / len(results["olt"]), 2)
            
            # Afficher la jauge
            st.plotly_chart(create_gauge_chart(olt_percent, f"% d'OLT anormaux"), use_container_width=True)
            
            st.metric("Nbr OLT anormaux / Total", f"{len(anomalous_olt)} / {len(results['olt'])}")
            
            if not anomalous_olt.empty:
                # Trouver les OLT les plus probl√©matiques
                worst_olt = anomalous_olt.sort_values('avg_isolation_forest_score').index[:3]
                
                # Cr√©er une table avec formatage color√© pour les OLT les plus anormaux
                st.markdown("#### OLT les plus anormaux:")
                for idx in worst_olt:
                    score = anomalous_olt.loc[idx, 'avg_isolation_forest_score']
                    color = 'red' if score < threshold_if else 'orange'
                    st.markdown(f"<span style='color:{color};font-weight:bold;font-size:16px'>‚Ä¢ {idx}</span> (score: {score:.3f})", unsafe_allow_html=True)
    
    # Ajouter une section pour un graphique synth√©tique des anomalies
    st.markdown("---")
    if st.session_state.df_with_anomalies is not None:
        st.subheader("Distribution des anomalies par type de n≈ìud")
        display_node_anomaly_chart()
    

# Fonction pour afficher une carte de chaleur des anomalies (Isolation Forest)
# def display_anomaly_heatmap():
#     if st.session_state.df_with_anomalies is None:
#         return
    
#     # Pr√©parer les donn√©es pour la heatmap
#     df_anomalies = st.session_state.df_with_anomalies
    
#     # Calculer le pourcentage d'anomalies par OLT et PEAG
#     cross_tab = pd.crosstab(
#         df_anomalies['peag_nro'], 
#         df_anomalies['olt_name'],
#         values=df_anomalies['anomaly_score'].apply(lambda x: 1 if x == -1 else 0),
#         aggfunc='mean'
#     ).fillna(0)
    
#     # Limiter √† 10 PEAG et 10 OLT maximum pour la lisibilit√©
#     if cross_tab.shape[0] > 10 or cross_tab.shape[1] > 10:
#         # Identifier les PEAG et OLT avec le plus d'anomalies
#         peag_anomaly_count = cross_tab.sum(axis=1).sort_values(ascending=False).head(10).index
#         olt_anomaly_count = cross_tab.sum(axis=0).sort_values(ascending=False).head(10).index
#         cross_tab = cross_tab.loc[peag_anomaly_count, olt_anomaly_count]
    
#     fig = go.Figure(data=go.Heatmap(
#         z=cross_tab.values * 100,  # En pourcentage
#         x=cross_tab.columns,
#         y=cross_tab.index,
#         colorscale='Reds',
#         colorbar=dict(title="% Anomalies")
#     ))
    
#     fig.update_layout(
#         title="Heatmap des anomalies par OLT et PEAG",
#         xaxis_title="OLT",
#         yaxis_title="PEAG",
#         height=400,
#         margin=dict(l=0, r=0, t=40, b=0)
#     )
    
#     st.plotly_chart(fig, use_container_width=True)

# Page d'insertion d'anomalies
def show_anomaly_insertion():
    st.header("üîß Insertion d'anomalies")
    
    st.write("""
    Cette section vous permet d'ins√©rer des anomalies simul√©es dans le r√©seau.
    S√©lectionnez les n≈ìuds, les variables de test et les valeurs √† ins√©rer.
    """)
    
    # Cr√©ation de tabs pour organiser l'insertion par type de n≈ìud
    tab1, tab2, tab3 = st.tabs(["Boucles", "PEAG", "OLT"])
    
    with tab1:
        st.subheader("Insertion d'anomalies dans les Boucles")
        col_names, col_vars, col_val, col_btn = st.columns([2, 2, 1, 1])
        
        with col_names:
            boucle_names = st.multiselect(
                "Boucles √† modifier", 
                options=sorted(list(st.session_state.lignes_1fev['boucle'].unique()))
            )
        with col_vars:
            var_test = st.multiselect(
                "Variables de test", 
                options=st.session_state.variables_test, 
                key='insertvar1'
            )
        with col_val:
            valeur_insertion = st.number_input(
                "Valeur √† ins√©rer", 
                value=0.0, 
                step=1.0, 
                key='insertion1'
            )
        with col_btn:
            if st.button("Ins√©rer anomalies", key="btn_inserer1"):
                st.write(f"Insertion d'une valeur de {round(valeur_insertion, 2)} dans {var_test} pour {len(boucle_names)} boucles")
                st.session_state.lignes_1fev = NodesChecker.add_anomalies(
                    st.session_state.lignes_1fev, 
                    'boucle', 
                    boucle_names,
                    var_test,
                    valeur_insertion
                )
                st.success(f"Anomalies ins√©r√©es avec succ√®s dans {len(boucle_names)} boucles!")
                # R√©initialiser le statut de d√©tection
                st.session_state.anomalies_detected = False
    
    with tab2:
        st.subheader("Insertion d'anomalies dans les PEAG")
        col_names, col_vars, col_val, col_btn = st.columns([2, 2, 1, 1])
        
        with col_names:
            peag_names = st.multiselect(
                "PEAG √† modifier", 
                options=sorted(list(st.session_state.lignes_1fev['peag_nro'].unique()))
            )
        with col_vars:
            var_test = st.multiselect(
                "Variables de test", 
                options=st.session_state.variables_test, 
                key='insertvar2'
            )
        with col_val:
            valeur_insertion = st.number_input(
                "Valeur √† ins√©rer", 
                value=0.0, 
                step=1.0, 
                key='insertion2'
            )
        with col_btn:
            if st.button("Ins√©rer anomalies", key="btn_inserer2"):
                st.write(f"Insertion d'une valeur de {round(valeur_insertion, 2)} dans {var_test} pour {len(peag_names)} PEAG")
                st.session_state.lignes_1fev = NodesChecker.add_anomalies(
                    st.session_state.lignes_1fev, 
                    'peag_nro', 
                    peag_names,
                    var_test,
                    valeur_insertion
                )
                st.success(f"Anomalies ins√©r√©es avec succ√®s dans {len(peag_names)} PEAG!")
                # R√©initialiser le statut de d√©tection
                st.session_state.anomalies_detected = False
    
    with tab3:
        st.subheader("Insertion d'anomalies dans les OLT")
        col_names, col_vars, col_val, col_btn = st.columns([2, 2, 1, 1])
        
        with col_names:
            olt_names = st.multiselect(
                "OLT √† modifier", 
                options=sorted(list(st.session_state.lignes_1fev['olt_name'].unique()))
            )
        with col_vars:
            var_test = st.multiselect(
                "Variables de test", 
                options=st.session_state.variables_test, 
                key='insertvar3'
            )
        with col_val:
            valeur_insertion = st.number_input(
                "Valeur √† ins√©rer", 
                value=0.0, 
                step=1.0, 
                key='insertion3'
            )
        with col_btn:
            if st.button("Ins√©rer anomalies", key="btn_inserer3"):
                st.write(f"Insertion d'une valeur de {round(valeur_insertion, 2)} dans {var_test} pour {len(olt_names)} OLT")
                st.session_state.lignes_1fev = NodesChecker.add_anomalies(
                    st.session_state.lignes_1fev, 
                    'olt_name', 
                    olt_names,
                    var_test,
                    valeur_insertion
                )
                st.success(f"Anomalies ins√©r√©es avec succ√®s dans {len(olt_names)} OLT!")
                # R√©initialiser le statut de d√©tection
                st.session_state.anomalies_detected = False
    
    # Bouton pour r√©initialiser les donn√©es
    st.markdown("---")
    if st.button("üîÑ R√©initialiser toutes les donn√©es", type="secondary"):
        st.session_state.lignes_1fev = st.session_state.lignes_1fev_copy.copy()
        st.session_state.anomalies_detected = False
        st.success("Toutes les donn√©es ont √©t√© r√©initialis√©es avec succ√®s!")


def show_detection_config():
    st.header("‚öôÔ∏è Configuration et lancement de la d√©tection d'anomalies")
  
    # Section des m√©thodes de d√©tection
    st.subheader("M√©thodes de d√©tection")
    
    # Utiliser plus d'espace horizontal entre les colonnes
    st.markdown("""
    <style>
    .stColumn > div {
        margin: 0 10px;
    }
    </style>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        use_unidimensional = st.checkbox("Approche unidimensionnelle", value=True)
        st.info("Analyse statistique des distributions de chaque m√©trique")
    
    with col2:
        use_isolation_forest = st.checkbox("Isolation Forest", value=True)
        st.info("D√©tection de points atypiques dans un espace multidimensionnel")
    
    with col3:
        use_mahalanobis = st.checkbox("Distance de Mahalanobis", value=True) 
        st.info("Mesure de l'√©loignement par rapport √† la distribution normale")

# Configuration des seuils
    st.subheader("Configuration des seuils")
    
    # Approche unidimensionnelle
    if use_unidimensional:
        st.markdown("##### Approche unidimensionnelle")
        unidim_threshold = st.slider(
            "Seuil (%) de rejet Œ± pour l'approche unidimensionnelle", 
            min_value=0.5, 
            max_value=20.0, 
            value=st.session_state.p_value_threshold, 
            step=0.05,
            format="%.1f"
        )
        st.session_state.p_value_threshold = unidim_threshold
        st.info(f"Les variables avec une p-value inf√©rieure √† {unidim_threshold}% seront consid√©r√©es comme anormales")
    
    # Isolation Forest
    if use_isolation_forest:
        st.markdown("##### Isolation Forest")
        if_threshold = st.slider(
            "Seuil de score d'anomalie pour Isolation Forest", 
            min_value=-1.0, 
            max_value=0.5, 
            value=st.session_state.isolation_forest_threshold, 
            step=0.05,
            format="%.2f"
        )
        st.session_state.isolation_forest_threshold = if_threshold
        st.info(f"Les observations avec un score inf√©rieur √† {if_threshold} seront consid√©r√©es comme anormales")
    
    # Mahalanobis
    if use_mahalanobis:
        st.markdown("##### Distance de Mahalanobis")
        
        # Stockage du seuil dans session_state s'il n'existe pas encore
        if "mahalanobis_threshold" not in st.session_state:
            st.session_state.mahalanobis_threshold = 0.01
            
        maha_threshold = st.slider(
            "Seuil de p-value pour Mahalanobis", 
            min_value=0.01, 
            max_value=10.0, 
            value=st.session_state.mahalanobis_threshold, 
            step=0.05,
            format="%.2f"
        )
        st.session_state.mahalanobis_threshold = maha_threshold
        st.info(f"Les observations avec une p-value inf√©rieure √† {maha_threshold} seront consid√©r√©es comme anormales")
    
    # Configuration de l'approche ensembliste
    st.subheader("Approche ensembliste")
    
    ensemble_method = st.radio(
        "M√©thode de combinaison des d√©tecteurs",
        options=["Vote majoritaire", "Union", "Intersection"],
        index=0,
        help="Vote majoritaire: anomalie si d√©tect√©e par au moins 2 m√©thodes\nUnion: anomalie si d√©tect√©e par au moins 1 m√©thode\nIntersection: anomalie si d√©tect√©e par toutes les m√©thodes"
    )
    
    if "ensemble_method" not in st.session_state:
        st.session_state.ensemble_method = "Vote majoritaire"
    st.session_state.ensemble_method = ensemble_method
    
    # Lancement de la d√©tection
    st.markdown("---")
    launch_col1, launch_col2 = st.columns(2)
    
    with launch_col1:
        launch_button_text = "üöÄ Lancer la d√©tection d'anomalies"
        if st.button(launch_button_text, type="primary", use_container_width=True):
            with st.spinner("D√©tection d'anomalies en cours ..."):
                # Stocker les m√©thodes s√©lectionn√©es
                st.session_state.use_unidimensional = use_unidimensional
                st.session_state.use_isolation_forest = use_isolation_forest
                st.session_state.use_mahalanobis = use_mahalanobis
                
                run_anomaly_detection()
                st.session_state.anomalies_detected = True
                st.success("D√©tection d'anomalies termin√©e avec succ√®s!")
                st.balloons()  # Animation pour c√©l√©brer la fin de l'analyse
    
    with launch_col2:
        if st.button("üîÑ R√©initialiser les param√®tres", use_container_width=True):
            st.session_state.p_value_threshold = 5.0
            st.session_state.isolation_forest_threshold = 0.0
            st.session_state.mahalanobis_threshold = 0.01
            st.session_state.ensemble_method = "Vote majoritaire"
            st.success("Param√®tres r√©initialis√©s aux valeurs par d√©faut")
            st.experimental_rerun()
            
    # Explication d√©taill√©e en bas de page
    with st.expander("Comprendre les seuils de d√©tection", expanded=False):
        st.markdown("""
        ### Guide des seuils de d√©tection

        #### Approche unidimensionnelle
        Le seuil repr√©sente le niveau de confiance pour rejeter l'hypoth√®se nulle (que les donn√©es sont normales).
        - **5%** : Valeur standard recommand√©e dans la plupart des cas
        - **1%** : D√©tection plus stricte, moins de faux positifs mais peut manquer des anomalies
        - **10%** : D√©tection plus sensible, capture plus d'anomalies potentielles avec plus de faux positifs

        #### Isolation Forest
        Le score d'anomalie est entre -1 et +1, o√π:
        - **-1** : Anomalie certaine
        - **0** : Cas limite
        - **+1** : Normal certain
        
        La valeur par d√©faut de 0 est un bon compromis. Une valeur n√©gative rend la d√©tection plus stricte.

        #### Distance de Mahalanobis
        La p-value indique la probabilit√© qu'une observation soit issue de la distribution normale.
        - **0.01** (1%) : Niveau standard de d√©tection
        - **0.001** (0.1%) : D√©tection tr√®s stricte, uniquement les anomalies extr√™mes
        - **0.05** (5%) : D√©tection plus sensible

        #### Approche ensembliste
        - **Vote majoritaire** : √âquilibre entre pr√©cision et sensibilit√©
        - **Union** : Maximise la sensibilit√© (capture toutes les anomalies possibles)
        - **Intersection** : Maximise la pr√©cision (minimise les faux positifs)
        """)
                   


# def show_impact_analysis():
#     st.subheader("üìâ Analyse d'impact potentiel")

#     # Initialisation
#     total_anomalies = 0
#     total_nodes = 0
#     anomalies_by_type = {"boucles": 0, "peag_nro": 0, "olt": 0}
#     client_counts = {}
#     degradation_metrics = {}

#     for node_type in ["boucles", "peag_nro", "olt"]:
#         df = st.session_state.results.get(node_type)
#         if df is not None:
#             total_nodes += len(df)
#             anomaly_count = df['is_anomaly'].sum() if 'is_anomaly' in df.columns else 0
#             total_anomalies += anomaly_count
#             anomalies_by_type[node_type] = anomaly_count

#             # Clients impact√©s
#             node_col = {"boucles": "boucle", "peag_nro": "peag_nro", "olt": "olt_name"}[node_type]
#             if 'nb_client_total' in st.session_state.lignes_1fev_copy.columns:
#                 anomalous_nodes = df[df['is_anomaly']].index.tolist() if 'is_anomaly' in df.columns else []
#                 total_clients = 0
#                 for node in anomalous_nodes:
#                     mask = st.session_state.lignes_1fev_copy[node_col] == node
#                     total_clients += st.session_state.lignes_1fev_copy.loc[mask, 'nb_client_total'].sum()
#                 client_counts[node_type] = total_clients
#             else:
#                 default_per_node = {"boucles": 10, "peag_nro": 25, "olt": 50}
#                 client_counts[node_type] = anomalies_by_type[node_type] * default_per_node[node_type]

#             # D√©gradations
#             if 'is_anomaly' in df.columns and df['is_anomaly'].sum() > 0:
#                 for metric, invert in [("avg_dns_time", False), ("avg_latence_scoring", False), ("avg_score_scoring", True)]:
#                     if metric in df.columns:
#                         normal = df[~df['is_anomaly']][metric].mean()
#                         anomaly = df[df['is_anomaly']][metric].mean()
#                         if pd.notna(normal) and normal > 0:
#                             delta = ((normal - anomaly) / normal) * 100 if invert else ((anomaly - normal) / normal) * 100
#                             degradation_metrics[f"{node_type}_{metric}"] = max(0, round(delta, 2))

#     # R√©sum√©s globaux
#     total_clients_impacted = int(sum(client_counts.values()))
#     overall_degradation = round(sum(degradation_metrics.values()) / len(degradation_metrics), 1) if degradation_metrics else 0
#     max_clients = max(1000, total_clients_impacted * 1.2)

#     # üîí Valeurs s√©curis√©es
#     total_clients_impacted = max(0, total_clients_impacted)
#     overall_degradation = max(0, min(100, overall_degradation))

#     # üìä Jauges
#     col1, col2 = st.columns(2)

#     with col1:
#         fig1 = go.Figure(go.Indicator(
#             mode="gauge+number",
#             value=total_clients_impacted,
#             title={'text': "Clients impact√©s", 'font': {'size': 18, 'color': 'white'}},
#             number={'font': {'size': 26, 'color': 'white'}},
#             gauge={
#                 'axis': {'range': [0, max_clients], 'tickwidth': 1, 'tickcolor': 'white'},
#                 'bar': {'color': "rgba(0,0,0,0)"},
#                 'steps': [
#                     {'range': [0, 200], 'color': '#3cb44b'},
#                     {'range': [200, 500], 'color': '#ff9a00'},
#                     {'range': [500, max_clients], 'color': '#e6194B'}
#                 ],
#                 'threshold': {'line': {'color': "red", 'width': 4}, 'value': 500}
#             }
#         ))
#         fig1.update_layout(
#             height=300,
#             paper_bgcolor="rgba(0,0,0,0)",
#             plot_bgcolor="rgba(0,0,0,0)",
#             font=dict(color="white")
#         )
#         st.markdown("&nbsp;", unsafe_allow_html=True)
#         st.plotly_chart(fig1, use_container_width=True)

#     with col2:
#         fig2 = go.Figure(go.Indicator(
#             mode="gauge+number",
#             value=overall_degradation,
#             title={'text': "D√©gradation du service", 'font': {'size': 18, 'color': 'white'}},
#             number={'suffix': "%", 'font': {'size': 26, 'color': 'white'}},
#             gauge={
#                 'axis': {'range': [0, 100], 'tickwidth': 1, 'tickcolor': 'white'},
#                 'bar': {'color': "rgba(0,0,0,0)"},
#                 'steps': [
#                     {'range': [0, 25], 'color': '#3cb44b'},
#                     {'range': [25, 50], 'color': '#ff9a00'},
#                     {'range': [50, 100], 'color': '#e6194B'}
#                 ],
#                 'threshold': {'line': {'color': "red", 'width': 4}, 'value': 75}
#             }
#         ))
#         fig2.update_layout(
#             height=300,
#             paper_bgcolor="rgba(0,0,0,0)",
#             plot_bgcolor="rgba(0,0,0,0)",
#             font=dict(color="white")
#         )
#         st.markdown("&nbsp;", unsafe_allow_html=True)
#         st.plotly_chart(fig2, use_container_width=True)

#     # üìä Barres de d√©gradation
#     if degradation_metrics:
#         st.subheader("üìä D√©tails de la d√©gradation par m√©trique")
#         metrics_df = pd.DataFrame({
#             'M√©trique': list(degradation_metrics.keys()),
#             'D√©gradation (%)': list(degradation_metrics.values())
#         }).sort_values("D√©gradation (%)", ascending=False)

#         fig = px.bar(
#             metrics_df,
#             y='M√©trique',
#             x='D√©gradation (%)',
#             orientation='h',
#             color='D√©gradation (%)',
#             color_continuous_scale=['green', 'yellow', 'red'],
#             title="D√©gradation par m√©trique et type de n≈ìud"
#         )
#         fig.update_layout(height=400)
#         st.markdown("&nbsp;", unsafe_allow_html=True)
#         st.plotly_chart(fig, use_container_width=True)

#     # üß† Bloc d'analyse final
#     st.markdown("""
#     <div style='background-color: #1e1e1e; padding: 20px; border-radius: 10px; margin-top: 20px;'>
#         <h4 style='color: #ff4d4d;'>‚ö†Ô∏è Cons√©quences potentielles si aucune action n'est prise</h4>
#         <ul style='margin-top: 10px; font-size: 16px; color: white;'>
#             <li><strong>Court terme (24-48h):</strong> Augmentation des temps de r√©ponse DNS et de latence</li>
#             <li><strong>Moyen terme (3-5 jours):</strong> D√©gradation du streaming, jeux en ligne, navigation</li>
#             <li><strong>Long terme (>1 semaine):</strong> Risque de coupures et hausse des appels au service client</li>
#         </ul>
#         <p style='margin-top: 15px; font-weight: bold; font-size: 16px; color: white;'>
#             Une intervention pr√©ventive est recommand√©e pour √©viter l‚Äôaggravation du probl√®me et prot√©ger la satisfaction client.
#         </p>
#     </div>
#     """, unsafe_allow_html=True)


# Fonction pour obtenir un label de gravit√© bas√© sur la p-value
def get_severity_label(p_value):
    if p_value < 0.001:
        return "Critique"
    elif p_value < 0.01:
        return "S√©v√®re"
    elif p_value < 0.05:
        return "Mod√©r√©"
    else:
        return "Faible"


def create_gauge_chart(percentage, title, ratio_text):
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=percentage,
        number={'suffix': "%", 'font': {'size': 24, 'color': 'white'}},
        domain={'x': [0, 1], 'y': [0.2, 1]},  # d√©caler vers le haut
        title={'text': title, 'font': {'size': 18, 'color': 'white'}},
        gauge={
            'axis': {
                'range': [0, 100],
                'tickwidth': 1,
                'tickcolor': "white",
                'tickfont': {'size': 12, 'color': 'white'}
            },
            'bar': {'color': "rgba(0,0,0,0)"},
            'bgcolor': "white",
            'borderwidth': 1,
            'bordercolor': "#ccc",
            'steps': [
                {'range': [0, 25], 'color': 'green'},
                {'range': [25, 50], 'color': 'yellow'},
                {'range': [50, 75], 'color': 'orange'},
                {'range': [75, 100], 'color': 'red'}
            ],
        }
    ))

    # Ajouter le ratio en dessous de la jauge
    fig.add_annotation(
        text=f"<b>{ratio_text}</b>",
        x=0.5,
        y=0.05,  # juste en bas
        showarrow=False,
        font=dict(size=16),
        xanchor="center"
    )

    fig.update_layout(
        height=260,
        margin=dict(l=10, r=10, t=40, b=10),
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)"
    )
    return fig


def show_anomaly_statistics():
    st.subheader("üìä Statistiques des anomalies d√©tect√©es")
    
    # Compter les anomalies par type de n≈ìud
    stats = {}
    for node_type, display_name in zip(["boucles", "peag_nro", "olt"], ["Boucles", "PEAG", "OLT"]):
        if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
            df = st.session_state.results[node_type]
            total_count = len(df)
            anomaly_count = df['is_anomaly'].sum() if 'is_anomaly' in df.columns else 0
            percentage = (anomaly_count / total_count * 100) if total_count > 0 else 0

            stats[display_name] = {
                "total": total_count,
                "anomalies": anomaly_count,
                "percentage": percentage
            }

    # Afficher les jauges dans des colonnes bien r√©parties
    cols = st.columns(len(stats), gap="large")

    for i, (display_name, data) in enumerate(stats.items()):
        with cols[i]:
            fig = create_gauge_chart(
                data['percentage'],
                f"{display_name} anormaux",
                f"{data['anomalies']} / {data['total']} {display_name}"
            )
            st.plotly_chart(fig, use_container_width=True)
            st.markdown(
                "<div style='text-align: center;'>&nbsp;</div>",
                unsafe_allow_html=True
            )


    # Graphique √† barres des m√©thodes
    st.subheader("üìå R√©partition des anomalies par m√©thode de d√©tection")

    method_stats = {
        "Unidimensionnelle": 0,
        "Isolation Forest": 0,
        "Mahalanobis": 0
    }

    for node_type in ["boucles", "peag_nro", "olt"]:
        if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
            df = st.session_state.results[node_type]
            threshold = st.session_state.p_value_threshold / 100

            # Compter par m√©thode
            if any(col.startswith('p_val_') for col in df.columns):
                unidim_count = (df.filter(like='p_val_') < threshold).any(axis=1).sum()
                method_stats["Unidimensionnelle"] += unidim_count

            if 'avg_isolation_forest_score' in df.columns:
                if_count = (df['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold).sum()
                method_stats["Isolation Forest"] += if_count

            if 'mahalanobis_anomaly' in df.columns:
                mah_count = df['mahalanobis_anomaly'].sum()
                method_stats["Mahalanobis"] += mah_count
            elif 'avg_mahalanobis_pvalue' in df.columns:
                mah_count = (df['avg_mahalanobis_pvalue'] < st.session_state.mahalanobis_threshold).sum()
                method_stats["Mahalanobis"] += mah_count

    # Afficher le graphique √† barres
    fig = px.bar(
        x=list(method_stats.keys()),
        y=list(method_stats.values()),
        labels={"x": "M√©thode de d√©tection", "y": "Nombre d'anomalies"},
        color=list(method_stats.keys()),
        color_discrete_map = {
            "Unidimensionnelle": "#fcae91",   # Rouge clair
            "Isolation Forest": "#fb6a4a",    # Rouge moyen
            "Mahalanobis": "#de2d26"          # Rouge fonc√©
        },
        text=list(method_stats.values())
    )

    fig.update_traces(textposition='outside')
    fig.update_layout(
        height=400,
        xaxis=dict(title=None),
        margin=dict(t=20, b=20, l=20, r=20),
        showlegend=False,
        font=dict(size=16),
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)"
    )

    st.plotly_chart(fig, use_container_width=True)


def display_equipment_summary_table():
    st.subheader("üìã Synth√®se par √©quipement")

    rows = []
    threshold = st.session_state.p_value_threshold / 100
    
    # Mapping pour assurer l'acc√®s correct aux colonnes
    column_mapping = {
        "boucles": "boucle",
        "peag_nro": "peag_nro",
        "olt": "olt_name"
    }
    
    for node_type, display_name in zip(["boucles", "peag_nro", "olt"], ["Boucle", "PEAG", "OLT"]):
        if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
            df = st.session_state.results[node_type]
            
            for idx, row in df.iterrows():
                # V√©rifier si c'est une anomalie
                is_anomaly = row.get('is_anomaly', False)
                
                if is_anomaly:
                    # Collecter les m√©triques en anomalie
                    abnormal_metrics = []
                    for col in df.columns:
                        if 'p_val_' in col and row[col] < threshold:
                            metric_name = col.replace('p_val_', '')
                            abnormal_metrics.append(metric_name)
                    
                    # V√©rifier les autres m√©thodes de d√©tection
                    detection_methods = []
                    if abnormal_metrics:
                        detection_methods.append("Unidimensionnelle")
                    
                    if 'avg_isolation_forest_score' in row and row['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold:
                        detection_methods.append("Isolation Forest")
                    
                    if 'mahalanobis_anomaly' in row and row['mahalanobis_anomaly'] == 1:
                        detection_methods.append("Mahalanobis")
                    
                    # D√©terminer les m√©triques touch√©es
                    metrics_text = ", ".join(abnormal_metrics) if abnormal_metrics else "N/A"
                    
                    # D√©terminer la gravit√© (3 classes seulement)
                    p_values = [row[col] for col in df.columns if 'p_val_' in col]
                    if p_values:
                        min_p_value = min(p_values)
                        if min_p_value < 0.01:  # Tr√®s faible p-value
                            severity = "Critique"
                        elif min_p_value < 0.05:  # Faible p-value
                            severity = "Mod√©r√©"
                        else:  # P-value plus √©lev√©e
                            severity = "Faible"
                    else:
                        # Si pas de p-values, utiliser isolation forest ou autre
                        if 'avg_isolation_forest_score' in row and row['avg_isolation_forest_score'] < -0.3:
                            severity = "Critique"
                        elif 'avg_isolation_forest_score' in row and row['avg_isolation_forest_score'] < 0:
                            severity = "Mod√©r√©"
                        else:
                            severity = "Faible"
                    
                    # D√©terminer le d√©partement
                    try:
                        col_name = column_mapping[node_type]
                        mask = st.session_state.lignes_1fev_copy[col_name] == idx
                        dept = st.session_state.lignes_1fev_copy.loc[mask, 'code_departement'].iloc[0] if any(mask) else "N/A"
                    except (KeyError, IndexError):
                        dept = "N/A"
                    
                    # Cr√©er l'entr√©e pour le tableau
                    rows.append({
                        "Type": display_name,
                        "Identifiant": idx,
                        "D√©partement": dept,
                        "Gravit√©": severity,
                        "M√©triques touch√©es": metrics_text,
                        "M√©thode(s) de d√©tection": ", ".join(detection_methods) if detection_methods else "Aucune"
                    })

    # Cr√©er le DataFrame de synth√®se
    if rows:
        df_summary = pd.DataFrame(rows)

        if len(df_summary) > 0:
            df_summary = df_summary.sort_values("Gravit√©", key=lambda x: x.map({"Critique": 0, "Mod√©r√©": 1, "Faible": 2}))

            def highlight_severity(val):
                color = ""
                if val == "Critique":
                    color = "#e63946"
                elif val == "Mod√©r√©":
                    color = "#f9844a"
                elif val == "Faible":
                    color = "#90be6d"
                return f'background-color: {color}; color: white'

            styled_df = df_summary.style.applymap(highlight_severity, subset=['Gravit√©'])

            st.dataframe(styled_df, use_container_width=True, hide_index=True)
        else:
            st.info("Aucune anomalie n'a √©t√© d√©tect√©e avec les seuils actuels.")
    else:
        st.info("Aucune donn√©e d'anomalie disponible. Veuillez lancer une d√©tection.")

def display_anomaly_details():
    st.subheader("üî¨ D√©tail d'une anomalie")

    # Collecter toutes les anomalies
    all_anomalies = []
    for node_type in ["boucles", "peag_nro", "olt"]:
        if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
            df = st.session_state.results[node_type]
            for idx in df.index:
                # V√©rifier si c'est une anomalie
                is_anomaly = False
                for col in df.columns:
                    if 'p_val_' in col and df.loc[idx, col] < st.session_state.p_value_threshold / 100:
                        is_anomaly = True
                        break
                if 'avg_isolation_forest_score' in df.columns and df.loc[idx, 'avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold:
                    is_anomaly = True
                
                if is_anomaly:
                    all_anomalies.append(f"{node_type.upper()} - {idx}")
    
    if not all_anomalies:
        st.info("Aucune anomalie d√©tect√©e pour l'instant. Lancez une d√©tection ou ajustez les seuils.")
        return
    
    selected = st.selectbox("Choisir une anomalie √† explorer", options=all_anomalies)

    if selected:
        node_type, node_id = selected.split(" - ")
        node_type = node_type.lower()
        df = st.session_state.results[node_type]
        row = df.loc[node_id]

        st.markdown(f"### üß† Anomalie sur {node_type.upper()} {node_id}")
        
        # M√©triques concern√©es
        affected_metrics = []
        for col in df.columns:
            if "p_val_" in col and row[col] < st.session_state.p_value_threshold / 100:
                metric_name = col.replace('p_val_', '')
                affected_metrics.append(metric_name)
        
        # Si aucune m√©trique sp√©cifique mais Isolation Forest d√©tecte une anomalie
        if not affected_metrics and 'avg_isolation_forest_score' in row and row['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold:
            st.markdown("- Anomalie multivari√©e d√©tect√©e par Isolation Forest")
            
        if 'avg_mahalanobis_distance' in row and 'mahalanobis_anomaly' in row and row['mahalanobis_anomaly'] == 1:
            st.markdown("- Anomalie d√©tect√©e par distance de Mahalanobis")
        

    # Bouton pour afficher toutes les observations
    if st.button("üìë Afficher toutes les observations de ce n≈ìud"):
        try:
            # Trouver la colonne correspondante
            column_mapping = {
                "boucles": "boucle",
                "peag_nro": "peag_nro",
                "olt": "olt_name"
            }
            
            col_name = column_mapping.get(node_type)
            if col_name in st.session_state.lignes_1fev_copy.columns:
                mask = st.session_state.lignes_1fev_copy[col_name] == node_id
                node_data = st.session_state.lignes_1fev_copy[mask]
                
                if len(node_data) > 0:
                    # Supprimer la colonne d'index si elle existe
                    if 'Unnamed: 0' in node_data.columns:
                        node_data = node_data.drop(columns=['Unnamed: 0'])
                    
                    st.dataframe(node_data, use_container_width=True, hide_index=True)
                else:
                    st.info(f"Aucune observation trouv√©e pour {node_type.upper()} {node_id}")
            else:
                st.warning(f"Colonne {col_name} non trouv√©e dans les donn√©es")
        except Exception as e:
            st.error(f"Erreur lors de l'acc√®s aux donn√©es: {str(e)}")
            st.info("Impossible d'afficher les observations d√©taill√©es")

    # Recommandation
    p_values = [row[col] for col in df.columns if 'p_val_' in col]
    if p_values:
        severity = get_severity_label(min(p_values))
        if severity == "Critique":
            st.error("üìå Recommandation : Intervention imm√©diate requise")
        elif severity == "S√©v√®re":
            st.warning("üìå Recommandation : Intervention pr√©ventive recommand√©e")
        else:
            st.info("üìå Recommandation : Surveillance recommand√©e")
    else:
        st.info("üìå Recommandation : Surveillance recommand√©e")

    # Graphiques des variables
    st.subheader("√âvolution des m√©triques")
    
    # Liste des variables √† afficher
    all_metrics = [
        'avg_dns_time', 
        'std_dns_time', 
        'avg_score_scoring', 
        'std_score_scoring', 
        'avg_latence_scoring', 
        'std_latence_scoring'
    ]
    
    # S√©lection des variables √† afficher
    selected_metrics = st.multiselect(
        "S√©lectionner les m√©triques √† visualiser",
        options=all_metrics,
        default=[all_metrics[0]] if all_metrics else [],
        key="metrics_multiselect"
    )
    
    if selected_metrics:
        # G√©n√©rer des donn√©es simul√©es
        import numpy as np
        
        # Cr√©er un DataFrame avec des indices num√©riques pour les dates
        days = list(range(1, 61))
        df_chart = pd.DataFrame({'Jour': days})
        
        # G√©n√©rer des donn√©es pour chaque m√©trique
        np.random.seed(42)  # Pour reproductibilit√©
        for metric in selected_metrics:
            # G√©n√©rer des donn√©es simul√©es avec une tendance et un peu de bruit
            if 'avg' in metric:
                base = 10  # Valeur de base
            else:
                base = 2  # Valeur de base pour std
            
            trend = np.linspace(0, 2, 60)  # Tendance croissante
            noise = np.random.normal(0, 0.5, 60)  # Bruit al√©atoire
            
            # Ajouter une anomalie vers la fin
            anomaly = np.zeros(60)
            # D√©tection automatique du point d'anomalie - dernier quartile avec une croissance soudaine
            anomaly_start = 45  # 75% des donn√©es
            anomaly[anomaly_start:] = np.linspace(0, 3, 60-anomaly_start)  # Anomalie croissante
            
            data = base + trend + noise + anomaly
            df_chart[metric] = data
        
        # Cr√©er un SEUL graphique avec toutes les m√©triques s√©lectionn√©es
        fig = go.Figure()
        
        # Couleurs distinctes pour chaque m√©trique
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
        
        # Ajouter chaque m√©trique comme une ligne s√©par√©e
        for i, metric in enumerate(selected_metrics):
            fig.add_trace(go.Scatter(
                x=df_chart['Jour'],
                y=df_chart[metric],
                mode='lines+markers',
                name=metric,
                line=dict(color=colors[i % len(colors)], width=2),
                marker=dict(size=6)
            ))
        
        # Ajouter une ligne verticale pour le d√©but de l'anomalie d√©tect√©e
        anomaly_day = 45  # Point o√π l'anomalie commence (75% des donn√©es)
        fig.add_vline(
            x=anomaly_day,
            line_dash="dash",
            line_color="red",
            annotation_text="D√©but probable de l'anomalie"
        )
        
        # Mise en page
        fig.update_layout(
            title="√âvolution des m√©triques s√©lectionn√©es (60 derniers jours)",
            xaxis_title="Jour",
            yaxis_title="Valeur",
            legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="left",
                x=0.01
            ),
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)



def show_results():
    st.header("üìà R√©sultats de la d√©tection d'anomalies")

    if not st.session_state.anomalies_detected:
        st.warning("Veuillez lancer la d√©tection avant d'acc√©der aux r√©sultats.")
        return
    
    # V√©rifier que nous avons des r√©sultats √† afficher
    has_results = False
    for node_type in ["boucles", "peag_nro", "olt"]:
        if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
            has_results = True
            break
    
    if not has_results:
        st.error("Aucun r√©sultat disponible. Veuillez relancer la d√©tection d'anomalies.")
        return

    # 1. Statistiques d'anomalies
    show_anomaly_statistics()
    
    # 2. Tableau de synth√®se par √©quipement
    display_equipment_summary_table()
    
    # 3. D√©tails par anomalie
    display_anomaly_details()
    
    # 4. Section d'analyse d'impact
    # show_impact_analysis()

# Page d'aide
def show_about():
    st.header("‚ÑπÔ∏è √Ä propos de l'application")
    
    # Cr√©er des onglets √† l'int√©rieur de la page √† propos
    about_tabs = st.tabs(["Pr√©sentation", "Fonctionnalit√©s", "M√©thodologie", "Aide", "√âquipe"])
    
    with about_tabs[0]:  # Pr√©sentation
        st.subheader("Pr√©sentation du projet")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.write("""
            Cette application de d√©tection d'anomalies r√©seau a √©t√© d√©velopp√©e dans le cadre du Challenge Nexialog & Universit√© Paris 1 Panth√©on-Sorbonne. 
            
            L'objectif est d'anticiper les probl√®mes sur le r√©seau SFR afin d'intervenir avant que les clients ne soient impact√©s par des interruptions de service.
            
            L'application analyse en temps r√©el les donn√©es des tests r√©seau effectu√©s sur les diff√©rents √©quipements (OLT, PEAG, boucles) et identifie les comportements anormaux gr√¢ce √† des techniques avanc√©es de d√©tection d'anomalies.
            """)
        
        with col2:
            st.image("images/SFR_logo.png", width=150)
            st.image("images/nexialog_logo.png", width=150)
    
    with about_tabs[1]:  # Fonctionnalit√©s
        st.subheader("Principales fonctionnalit√©s")
        
        st.markdown("""
        #### üìä √âtat du r√©seau
        - Visualisation des statistiques globales du r√©seau
        - Aper√ßu des performances des m√©triques cl√©s
        - Distribution des variables de test
        
        #### üîß Insertion d'anomalies
        - Simulation d'anomalies sur diff√©rents types de n≈ìuds
        - Modification des valeurs de test pour tester les algorithmes
        
        #### ‚öôÔ∏è Configuration et d√©tection
        - Param√©trage des seuils de d√©tection
        - S√©lection des m√©thodes de d√©tection (unidimensionnelle, Isolation Forest, Mahalanobis)
        - Configuration de l'approche ensembliste
        
        #### üìà R√©sultats
        - Visualisation des anomalies d√©tect√©es
        - Analyse par type de n≈ìud (boucles, PEAG, OLT)
        - Visualisation 3D des anomalies multidimensionnelles
        """)
    
    with about_tabs[2]:  # M√©thodologie
        st.subheader("M√©thodologie de d√©tection")
        
        st.markdown("""
        Notre application utilise trois approches compl√©mentaires pour d√©tecter les anomalies r√©seau:
        
        #### 1. Approche unidimensionnelle
        Cette m√©thode analyse chaque m√©trique ind√©pendamment:
        - Applique un filtrage Hodrick-Prescott pour s√©parer tendance et cycles
        - Estime la distribution empirique via kernel density estimation (KDE)
        - Calcule des p-values pour d√©terminer si une observation est anormale
        - Utilise le test de Fisher pour combiner les p-values par n≈ìud
        
        #### 2. Isolation Forest
        Approche multidimensionnelle qui:
        - Construit des arbres de d√©cision qui isolent les observations
        - D√©tecte les anomalies comme les points n√©cessitant moins d'√©tapes d'isolation
        - Fonctionne sans hypoth√®se pr√©alable sur la distribution des donn√©es
        - Est particuli√®rement efficace pour les jeux de donn√©es √† grande dimension
        
        #### 3. Distance de Mahalanobis
        M√©thode statistique robuste qui:
        - Mesure la distance entre un point et la distribution globale
        - Prend en compte les corr√©lations entre variables
        - Utilise l'estimateur MCD (Minimum Covariance Determinant) pour la robustesse
        - Effectue une normalisation pour comparer les scores entre cha√Ænes techniques
        """)
        
    
    with about_tabs[3]:  # Aide
        st.subheader("Guide d'utilisation")
        
        st.markdown("""
        ### Comment utiliser l'application
        
        #### Configuration et lancement de la d√©tection
        1. Naviguez vers l'onglet "Configuration et d√©tection"
        2. S√©lectionnez les m√©thodes de d√©tection souhait√©es
        3. Ajustez les seuils de d√©tection selon vos besoins
        4. Choisissez la m√©thode de combinaison pour l'approche ensembliste
        5. Cliquez sur "Lancer la d√©tection d'anomalies"
        
        #### Interpr√©tation des r√©sultats
        - **p-value faible**: Plus la p-value est faible, plus l'anomalie est significative
        - **Score Isolation Forest n√©gatif**: Les scores proches de -1 sont les plus anormaux
        - **Distance de Mahalanobis √©lev√©e**: Indique un √©loignement de la distribution normale
        
        #### Conseils pratiques
        - Commencez par un seuil de p-value de 5% (0.05)
        - Pour l'Isolation Forest, un seuil de 0 est g√©n√©ralement appropri√©
        - Pour Mahalanobis, un seuil de 0.01 est recommand√©
        - Utilisez le vote majoritaire pour combiner les m√©thodes

        #### Insertion d'anomalies
        1. Naviguez vers l'onglet "Insertion d'anomalies"
        2. S√©lectionnez le type de n≈ìud (Boucle, PEAG ou OLT)
        3. Choisissez les n≈ìuds sp√©cifiques √† modifier
        4. S√©lectionnez les variables de test √† modifier
        5. Entrez la valeur √† ajouter
        6. Cliquez sur "Ins√©rer anomalies"                   
        """)
        
        # Afficher une FAQ
        with st.expander("Questions fr√©quentes"):
            st.markdown("""
            **Q: Comment savoir quelle m√©thode de d√©tection choisir?**  
            R: L'approche unidimensionnelle est meilleure pour identifier les anomalies sur des m√©triques sp√©cifiques. Isolation Forest et Mahalanobis sont plus adapt√©s pour d√©tecter des anomalies dans des combinaisons de variables.
            
            **Q: Que faire en cas d'anomalie d√©tect√©e?**  
            R: V√©rifiez d'abord la gravit√© et les m√©triques concern√©es. Pour les anomalies critiques, une intervention technique rapide est recommand√©e.
            
            **Q: Les faux positifs sont-ils possibles?**  
            R: Oui. C'est pourquoi nous utilisons plusieurs m√©thodes de d√©tection et une approche ensembliste pour r√©duire ce risque.
            """)
    

    with about_tabs[4]:  # √âquipe
        st.subheader("√âquipe de d√©veloppement")
        
        st.write("Challenge r√©alis√© par l'√©quipe du Master MOSEF de l'Universit√© Paris 1 Panth√©on-Sorbonne:")

        team_data = [
            {
                "nom": "Chahla Tarmoun",
                "role": "Data Scientist chez SG Corporate & Investment Banking",
                "linkedin": "https://www.linkedin.com/in/chahla-tarmoun-4b546a160/"
            },
            {
                "nom": "Louis Lebreton",
                "role": "Data Scientist chez Cr√©dit Agricole Leasing & Factoring",
                "linkedin": "https://www.linkedin.com/in/louis-lebreton-17418a1b7/"
            },
            {
                "nom": "Alexis Christien",
                "role": "Data Scientist chez Sogeti",
                "linkedin": "https://www.linkedin.com/in/alexis-christien/"
            },
            {
                "nom": "Issame Abdeljalil",
                "role": "Data Scientist chez Cr√©dit Agricole Leasing & Factoring",
                "linkedin": "https://www.linkedin.com/in/issameabdeljalil/"
            }
        ]

        # Affichage avec ic√¥nes cliquables
        for member in team_data:
            col1, col2 = st.columns([4, 1])
            with col1:
                st.markdown(f"**{member['nom']}**  \n{member['role']}")
            with col2:
                st.markdown(
                    f"<a href='{member['linkedin']}' target='_blank'>"
                    f"<img src='https://cdn-icons-png.flaticon.com/512/174/174857.png' width='24' style='margin-top: 6px'/>"
                    f"</a>",
                    unsafe_allow_html=True
                )

        st.write("### Remerciements")
        st.write("Nous tenons √† remercier les √©quipes de SFR et Nexialog pour leur encadrement et les donn√©es fournies pour ce challenge.")

# # Fonction pour ex√©cuter la d√©tection d'anomalies
# def run_anomaly_detection():
    
    
#     # Instance de classe : recherche les noeuds anormaux
#     nc = NodesChecker()
#     # Calcul des p_values √† partir des distributions empiriques
#     lignes_1fev_with_pval = NodesChecker.add_p_values(
#         st.session_state.lignes_1fev.copy(), 
#         st.session_state.dict_distribution_test
#     )

#     # application de l'isolation forest
#     lignes_1fev_with_if_scores = st.session_state.isolation_forest.predict(st.session_state.lignes_1fev)
#     st.session_state.df_with_anomalies = lignes_1fev_with_if_scores
    
#     # Boucles
#     # p values
#     df_p_values_boucle = nc.get_df_fisher_p_values(
#         lignes_1fev_with_pval,
#         node_type='boucle',
#         p_values=st.session_state.p_values_col
#     )
#     # scores d'isolation forest
#     df_if_scores_boucle = nc.get_if_scores_by_node(lignes_1fev_with_if_scores, node_type = 'boucle')
#     df_results_boucle = pd.merge(df_p_values_boucle, 
#                                   df_if_scores_boucle[['avg_isolation_forest_score', 
#                                                        'majority_anomaly_score',
#                                                        'anomaly_percentage',
#                                                        'total_samples',
#                                                        'anomaly_count']], 
#                                   how='left', 
#                                   left_index=True, right_index=True)

#     st.session_state.results["boucles"] = df_results_boucle
    
#     # Filtrer les boucles d√©faillantes
#     boucles_defaillantes = df_results_boucle[df_results_boucle['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold].index
    
#     if len(boucles_defaillantes) > 0:
#         lignes_1fev_with_pval = lignes_1fev_with_pval[~lignes_1fev_with_pval['boucle'].isin(boucles_defaillantes)]
#         lignes_1fev_with_if_scores = lignes_1fev_with_if_scores[~lignes_1fev_with_if_scores['boucle'].isin(boucles_defaillantes)]
    
#     # PEAG
#     # p values
#     df_p_values_peag = nc.get_df_fisher_p_values(
#         lignes_1fev_with_pval,
#         node_type='peag_nro',
#         p_values=st.session_state.p_values_col
#     )
#     # scores d'isolation forest
#     df_if_scores_peag = nc.get_if_scores_by_node(lignes_1fev_with_if_scores, node_type = 'peag_nro')
#     df_results_peag = pd.merge(df_p_values_peag, 
#                                   df_if_scores_peag[['avg_isolation_forest_score', 
#                                                        'majority_anomaly_score',
#                                                        'anomaly_percentage',
#                                                        'total_samples',
#                                                        'anomaly_count']], 
#                                   how='left', 
#                                   left_index=True, right_index=True)
#     st.session_state.results["peag_nro"] = df_results_peag
    
#     # Filtrer les PEAG d√©faillants
#     peag_defaillants = df_results_peag[df_results_peag['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold].index
   
#     if len(peag_defaillants) > 0:
#         lignes_1fev_with_pval = lignes_1fev_with_pval[~lignes_1fev_with_pval['peag_nro'].isin(peag_defaillants)]
#         lignes_1fev_with_if_scores = lignes_1fev_with_if_scores[~lignes_1fev_with_if_scores['peag_nro'].isin(peag_defaillants)]
    
#     # OLT
#     # p values
#     df_p_values_olt = nc.get_df_fisher_p_values(
#         lignes_1fev_with_pval,
#         node_type='olt_name',
#         p_values=st.session_state.p_values_col
#     )
#     # scores d'isolation forest
#     df_if_scores_olt = nc.get_if_scores_by_node(lignes_1fev_with_if_scores, node_type = 'olt_name')
#     df_results_olt = pd.merge(df_p_values_olt, 
#                                   df_if_scores_olt[['avg_isolation_forest_score', 
#                                                        'majority_anomaly_score',
#                                                        'anomaly_percentage',
#                                                        'total_samples',
#                                                        'anomaly_count']], 
#                                   how='left', 
#                                   left_index=True, right_index=True)
#     st.session_state.results["olt"] = df_results_olt

#     # Appliquer la m√©thode ensembliste choisie sur tous les r√©sultats
#     if st.session_state.ensemble_method != "Vote majoritaire" and st.session_state.ensemble_method != "Union" and st.session_state.ensemble_method != "Intersection":
#         st.session_state.ensemble_method = "Vote majoritaire"  # Valeur par d√©faut
    
#     # Parcourir tous les n≈ìuds et appliquer la m√©thode ensembliste
#     for node_type in ["boucles", "peag_nro", "olt"]:
#         if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
#             df = st.session_state.results[node_type]
            
#             # Identifier les anomalies selon chaque m√©thode
#             unidim_columns = [col for col in df.columns if 'p_val_' in col]
#             threshold = st.session_state.p_value_threshold / 100
            
#             # Cr√©er des masques pour chaque m√©thode
#             unidim_mask = pd.Series(False, index=df.index)
#             if unidim_columns:
#                 unidim_mask = (df[unidim_columns] < threshold).any(axis=1)
            
#             isolation_mask = pd.Series(False, index=df.index)
#             if 'avg_isolation_forest_score' in df.columns:
#                 isolation_mask = df['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold
            
#             mahalanobis_mask = pd.Series(False, index=df.index)
#             if 'mahalanobis_anomaly' in df.columns:
#                 mahalanobis_mask = df['mahalanobis_anomaly'] == 1
#             elif 'avg_mahalanobis_distance' in df.columns and 'mahalanobis_pvalue' in df.columns:
#                 mahalanobis_mask = df['mahalanobis_pvalue'] < st.session_state.mahalanobis_threshold
            
#             # Appliquer la m√©thode d'ensemble choisie
#             if st.session_state.ensemble_method == "Vote majoritaire":
#                 # Une anomalie est d√©tect√©e si au moins 2 m√©thodes la d√©tectent
#                 final_mask = (unidim_mask.astype(int) + isolation_mask.astype(int) + mahalanobis_mask.astype(int)) >= 2
#                 df['is_anomaly'] = final_mask
#             elif st.session_state.ensemble_method == "Union":
#                 # Une anomalie est d√©tect√©e si au moins une m√©thode la d√©tecte
#                 df['is_anomaly'] = unidim_mask | isolation_mask | mahalanobis_mask
#             elif st.session_state.ensemble_method == "Intersection":
#                 # Une anomalie est d√©tect√©e si toutes les m√©thodes la d√©tectent
#                 methods_used = sum([len(unidim_columns) > 0, 'avg_isolation_forest_score' in df.columns, 
#                                    'mahalanobis_anomaly' in df.columns or 'avg_mahalanobis_distance' in df.columns])
#                 if methods_used >= 2:  # Au moins 2 m√©thodes utilis√©es
#                     active_masks = []
#                     if len(unidim_columns) > 0:
#                         active_masks.append(unidim_mask)
#                     if 'avg_isolation_forest_score' in df.columns:
#                         active_masks.append(isolation_mask)
#                     if 'mahalanobis_anomaly' in df.columns or 'avg_mahalanobis_distance' in df.columns:
#                         active_masks.append(mahalanobis_mask)
                    
#                     final_mask = active_masks[0]
#                     for mask in active_masks[1:]:
#                         final_mask = final_mask & mask
                    
#                     df['is_anomaly'] = final_mask
#                 else:
#                     df['is_anomaly'] = unidim_mask | isolation_mask | mahalanobis_mask  # Default to union if only one method
            
#             # Mettre √† jour le DataFrame r√©sultant
#             st.session_state.results[node_type] = df


def run_anomaly_detection():
    """
    Fonction qui ex√©cute la d√©tection d'anomalies avec les m√©thodes s√©lectionn√©es 
    et les param√®tres configur√©s
    """
    
    # Instance de NodesChecker
    nc = NodesChecker()
    
    # 1. M√âTHODE UNIDIMENSIONNELLE
    if st.session_state.use_unidimensional:
        # Calcul des p-values √† partir des distributions empiriques
        lignes_1fev_with_pval = nc.add_p_values(
            st.session_state.lignes_1fev.copy(), 
            st.session_state.dict_distribution_test
        )
    else:
        # Cr√©er une copie sans p-values si la m√©thode n'est pas utilis√©e
        lignes_1fev_with_pval = st.session_state.lignes_1fev.copy()
    
    # 2. ISOLATION FOREST
    if st.session_state.use_isolation_forest:
        # Application de l'isolation forest
        detector_if = MultiIsolationForestDetector(chain_id_col='name')
        detector_if.load_models(lignes_1fev_with_pval)
        lignes_1fev_with_if_scores = detector_if.predict(lignes_1fev_with_pval)
        
        # Stocker pour r√©f√©rence
        st.session_state.df_with_anomalies = lignes_1fev_with_if_scores
    else:
        # Sans Isolation Forest, utiliser les donn√©es pr√©c√©dentes
        lignes_1fev_with_if_scores = lignes_1fev_with_pval
        st.session_state.df_with_anomalies = lignes_1fev_with_pval
    
    # 3. MAHALANOBIS
    if st.session_state.use_mahalanobis:
        detector_mah = MahalanobisDetector(chain_id_col='name')
        detector_mah.load_models(lignes_1fev_with_pval)
        lignes_1fev_with_mah_scores = detector_mah.predict(
            lignes_1fev_with_pval, 
            threshold=st.session_state.mahalanobis_threshold
        )
    else:
        # Sans Mahalanobis, utiliser les donn√©es pr√©c√©dentes
        lignes_1fev_with_mah_scores = lignes_1fev_with_if_scores
    
    # 4. EXTRACTION DES R√âSULTATS PAR TYPE DE N≈íUD
    
    # Valeurs p pour les boucles
    df_p_values_boucle = nc.get_df_fisher_p_values(
        lignes_1fev_with_pval,
        node_type='boucle',
        p_values=st.session_state.p_values_col
    )
    
    # Scores d'isolation forest pour les boucles
    if st.session_state.use_isolation_forest:
        df_if_scores_boucle = nc.get_if_scores_by_node(lignes_1fev_with_if_scores, node_type='boucle')
    else:
        df_if_scores_boucle = pd.DataFrame(index=df_p_values_boucle.index)
    
    # Scores de Mahalanobis pour les boucles
    if st.session_state.use_mahalanobis:
        df_mah_scores_boucle = nc.get_mahalanobis_scores_by_node(lignes_1fev_with_mah_scores, node_type='boucle')
    else:
        df_mah_scores_boucle = pd.DataFrame(index=df_p_values_boucle.index)
    
    # Fusion des r√©sultats pour les boucles
    df_results_boucle = df_p_values_boucle
    
    if not df_if_scores_boucle.empty:
        columns_to_merge = ['avg_isolation_forest_score', 'majority_anomaly_score',
                           'anomaly_percentage', 'total_samples', 'anomaly_count']
        present_columns = [col for col in columns_to_merge if col in df_if_scores_boucle.columns]
        if present_columns:
            df_results_boucle = pd.merge(
                df_results_boucle,
                df_if_scores_boucle[present_columns],
                how='left',
                left_index=True,
                right_index=True
            )
    
    if not df_mah_scores_boucle.empty:
        columns_to_merge = ['avg_mahalanobis_distance', 'avg_mahalanobis_pvalue',
                           'mahalanobis_anomaly_percentage', 'mahalanobis_anomaly_count']
        present_columns = [col for col in columns_to_merge if col in df_mah_scores_boucle.columns]
        if present_columns:
            df_results_boucle = pd.merge(
                df_results_boucle,
                df_mah_scores_boucle[present_columns],
                how='left',
                left_index=True,
                right_index=True
            )
    
    # R√©p√©ter le m√™me processus pour PEAG et OLT
    
    # PEAG
    df_p_values_peag = nc.get_df_fisher_p_values(
        lignes_1fev_with_pval,
        node_type='peag_nro',
        p_values=st.session_state.p_values_col
    )
    
    if st.session_state.use_isolation_forest:
        df_if_scores_peag = nc.get_if_scores_by_node(lignes_1fev_with_if_scores, node_type='peag_nro')
    else:
        df_if_scores_peag = pd.DataFrame(index=df_p_values_peag.index)
    
    if st.session_state.use_mahalanobis:
        df_mah_scores_peag = nc.get_mahalanobis_scores_by_node(lignes_1fev_with_mah_scores, node_type='peag_nro')
    else:
        df_mah_scores_peag = pd.DataFrame(index=df_p_values_peag.index)
    
    df_results_peag = df_p_values_peag
    
    if not df_if_scores_peag.empty:
        columns_to_merge = ['avg_isolation_forest_score', 'majority_anomaly_score',
                           'anomaly_percentage', 'total_samples', 'anomaly_count']
        present_columns = [col for col in columns_to_merge if col in df_if_scores_peag.columns]
        if present_columns:
            df_results_peag = pd.merge(
                df_results_peag,
                df_if_scores_peag[present_columns],
                how='left',
                left_index=True,
                right_index=True
            )
    
    if not df_mah_scores_peag.empty:
        columns_to_merge = ['avg_mahalanobis_distance', 'avg_mahalanobis_pvalue',
                           'mahalanobis_anomaly_percentage', 'mahalanobis_anomaly_count']
        present_columns = [col for col in columns_to_merge if col in df_mah_scores_peag.columns]
        if present_columns:
            df_results_peag = pd.merge(
                df_results_peag,
                df_mah_scores_peag[present_columns],
                how='left',
                left_index=True,
                right_index=True
            )
    
    # OLT
    df_p_values_olt = nc.get_df_fisher_p_values(
        lignes_1fev_with_pval,
        node_type='olt_name',
        p_values=st.session_state.p_values_col
    )
    
    if st.session_state.use_isolation_forest:
        df_if_scores_olt = nc.get_if_scores_by_node(lignes_1fev_with_if_scores, node_type='olt_name')
    else:
        df_if_scores_olt = pd.DataFrame(index=df_p_values_olt.index)
    
    if st.session_state.use_mahalanobis:
        df_mah_scores_olt = nc.get_mahalanobis_scores_by_node(lignes_1fev_with_mah_scores, node_type='olt_name')
    else:
        df_mah_scores_olt = pd.DataFrame(index=df_p_values_olt.index)
    
    df_results_olt = df_p_values_olt
    
    if not df_if_scores_olt.empty:
        columns_to_merge = ['avg_isolation_forest_score', 'majority_anomaly_score',
                           'anomaly_percentage', 'total_samples', 'anomaly_count']
        present_columns = [col for col in columns_to_merge if col in df_if_scores_olt.columns]
        if present_columns:
            df_results_olt = pd.merge(
                df_results_olt,
                df_if_scores_olt[present_columns],
                how='left',
                left_index=True,
                right_index=True
            )
    
    if not df_mah_scores_olt.empty:
        columns_to_merge = ['avg_mahalanobis_distance', 'avg_mahalanobis_pvalue',
                           'mahalanobis_anomaly_percentage', 'mahalanobis_anomaly_count']
        present_columns = [col for col in columns_to_merge if col in df_mah_scores_olt.columns]
        if present_columns:
            df_results_olt = pd.merge(
                df_results_olt,
                df_mah_scores_olt[present_columns],
                how='left',
                left_index=True,
                right_index=True
            )
    
    # 5. STOCKAGE DES R√âSULTATS
    st.session_state.results = {
        "boucles": df_results_boucle,
        "peag_nro": df_results_peag,
        "olt": df_results_olt
    }
    
    # 6. APPLICATION DE LA M√âTHODE ENSEMBLISTE
    if st.session_state.ensemble_method not in ["Vote majoritaire", "Union", "Intersection"]:
        st.session_state.ensemble_method = "Vote majoritaire"  # Valeur par d√©faut
    
    # Parcourir tous les n≈ìuds et appliquer la m√©thode ensembliste
    for node_type in ["boucles", "peag_nro", "olt"]:
        if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
            df = st.session_state.results[node_type]
            
            # Identifier les anomalies selon chaque m√©thode
            unidim_columns = [col for col in df.columns if 'p_val_' in col]
            threshold = st.session_state.p_value_threshold / 100
            
            # Cr√©er des masques pour chaque m√©thode
            unidim_mask = pd.Series(False, index=df.index)
            if unidim_columns:
                unidim_mask = (df[unidim_columns] < threshold).any(axis=1)
            
            isolation_mask = pd.Series(False, index=df.index)
            if 'avg_isolation_forest_score' in df.columns:
                isolation_mask = df['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold
            
            mahalanobis_mask = pd.Series(False, index=df.index)
            if 'mahalanobis_anomaly' in df.columns:
                mahalanobis_mask = df['mahalanobis_anomaly'] == 1
            elif 'avg_mahalanobis_pvalue' in df.columns:
                mahalanobis_mask = df['avg_mahalanobis_pvalue'] < st.session_state.mahalanobis_threshold
            
            # Compter les m√©thodes actives
            methods_used = []
            if len(unidim_columns) > 0:
                methods_used.append(unidim_mask)
            if 'avg_isolation_forest_score' in df.columns:
                methods_used.append(isolation_mask)
            if 'mahalanobis_anomaly' in df.columns or 'avg_mahalanobis_pvalue' in df.columns:
                methods_used.append(mahalanobis_mask)
            
            # Appliquer la m√©thode d'ensemble choisie
            if len(methods_used) > 0:
                if st.session_state.ensemble_method == "Vote majoritaire" and len(methods_used) >= 2:
                    # Convertir en entiers et sommer
                    vote_sum = sum(mask.astype(int) for mask in methods_used)
                    # Anomalie si au moins la moiti√© des m√©thodes disponibles la d√©tectent
                    min_votes = max(2, len(methods_used) // 2 + 1)  # Au moins 2 votes ou la majorit√©
                    final_mask = vote_sum >= min_votes
                elif st.session_state.ensemble_method == "Intersection" and len(methods_used) >= 2:
                    # Intersection: toutes les m√©thodes doivent √™tre d'accord
                    final_mask = methods_used[0]
                    for mask in methods_used[1:]:
                        final_mask = final_mask & mask
                else:
                    # Union (par d√©faut): au moins une m√©thode d√©tecte
                    final_mask = methods_used[0] if methods_used else pd.Series(False, index=df.index)
                    for mask in methods_used[1:]:
                        final_mask = final_mask | mask
                
                # Ajouter la colonne de d√©cision finale
                df['is_anomaly'] = final_mask
            else:
                # Aucune m√©thode active
                df['is_anomaly'] = pd.Series(False, index=df.index)
            
            # Mettre √† jour le DataFrame r√©sultant
            st.session_state.results[node_type] = df
    
    # 7. AFFICHER LES STATISTIQUES DE D√âTECTION
    method_counts = {
        "Unidimensionnelle": 0,
        "Isolation Forest": 0,
        "Mahalanobis": 0,
        "Total d'anomalies": 0
    }
    
    for node_type in ["boucles", "peag_nro", "olt"]:
        if node_type in st.session_state.results and st.session_state.results[node_type] is not None:
            df = st.session_state.results[node_type]
            
            # Compter par m√©thode et au total
            if len([col for col in df.columns if 'p_val_' in col]) > 0:
                unidim_count = (df.filter(like='p_val_') < threshold).any(axis=1).sum()
                method_counts["Unidimensionnelle"] += unidim_count
            
            if 'avg_isolation_forest_score' in df.columns:
                if_count = (df['avg_isolation_forest_score'] < st.session_state.isolation_forest_threshold).sum()
                method_counts["Isolation Forest"] += if_count
            
            if 'mahalanobis_anomaly' in df.columns:
                mah_count = df['mahalanobis_anomaly'].sum()
                method_counts["Mahalanobis"] += mah_count
            elif 'avg_mahalanobis_pvalue' in df.columns:
                mah_count = (df['avg_mahalanobis_pvalue'] < st.session_state.mahalanobis_threshold).sum()
                method_counts["Mahalanobis"] += mah_count
            
            if 'is_anomaly' in df.columns:
                method_counts["Total d'anomalies"] += df['is_anomaly'].sum()
    
    # Afficher un r√©sum√© des r√©sultats
    st.session_state.anomalies_detected = True
    
    # Retourner les statistiques pour r√©f√©rence
    return method_counts

def merge_results(df_p_values, df_if_scores, df_maha_scores):
    """
    Fusionne les r√©sultats des diff√©rentes m√©thodes de d√©tection
    """
    # Commencer avec le DataFrame non vide ou cr√©er un nouveau DataFrame vide
    if not df_p_values.empty:
        result = df_p_values.copy()
    elif not df_if_scores.empty:
        result = df_if_scores.copy()
    elif not df_maha_scores.empty:
        result = df_maha_scores.copy()
    else:
        return pd.DataFrame()  # Retourner un DataFrame vide si tous sont vides
    
    # Fusionner avec les scores d'isolation forest si disponibles
    if not df_if_scores.empty:
        result = pd.merge(
            result, 
            df_if_scores, 
            how='outer', 
            left_index=True, 
            right_index=True
        )
    
    # Fusionner avec les scores de Mahalanobis si disponibles
    if not df_maha_scores.empty:
        result = pd.merge(
            result, 
            df_maha_scores, 
            how='outer', 
            left_index=True, 
            right_index=True
        )
    
    return result

# Fonction pour cr√©er un graphique 3D pour l'approche unidimensionnelle
def create_3d_plot(test_name, variables_test):
    row_to_plot = st.session_state.lignes_1fev[st.session_state.lignes_1fev['name'] == test_name]
    
    # Axes x et y (distribution empirique)
    x = np.array(st.session_state.dict_distribution_test[variables_test[0]][test_name])
    y = np.array(st.session_state.dict_distribution_test[variables_test[1]][test_name])
    
    # Bornes de la grille
    xmin, xmax = x.min(), x.max()
    ymin, ymax = y.min(), y.max()
    
    # Grille 2D (100x100 points)
    X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]
    positions = np.vstack([X.ravel(), Y.ravel()])
    
    # Estimation densit√© empirique avec gaussian_kde
    values = np.vstack([x, y])
    kernel = gaussian_kde(values)
    Z = np.reshape(kernel(positions).T, X.shape)
    
    x_point = float(row_to_plot[variables_test[0]])
    y_point = float(row_to_plot[variables_test[1]])
    
    # Calculer la densit√© √† ce point pr√©cis
    point_position = np.array([[x_point], [y_point]])
    z_point = float(kernel(point_position))
    
    # Figure 3D
    fig = go.Figure(data=[go.Surface(x=X, y=Y, z=Z, colorscale='Plasma')])
    fig.update_layout(
        title=f'Distribution Empirique 3D de {test_name}',
        scene=dict(
            xaxis_title=variables_test[0],
            yaxis_title=variables_test[1],
            zaxis_title='Densit√©'
        ),
        width=900,
        height=700,
        margin=dict(l=0, r=0, b=0, t=30)
    )
    
    # Marqueur pour le point actuel
    is_anomaly = z_point < 0.05
    text = "Anomalie" if is_anomaly else ""
    marker_color = 'red' if is_anomaly else 'green'
    
    fig.add_trace(go.Scatter3d(
        x=[x_point],
        y=[y_point],
        z=[z_point + 0.05],  # L√©g√®rement au-dessus pour la visibilit√©
        mode='markers+text',
        text=[text],
        marker=dict(size=8, color=marker_color),
        textposition="top center"
    ))
    
    return fig

# Main pour organisation de l'interface
def main():

    st.markdown("""
        <style>
        /* Augmente la taille des titres des onglets */
        .stTabs [role="tab"] {
            font-size: 18px !important;
            font-weight: 600;
            padding: 10px 20px;
        }
        </style>
    """, unsafe_allow_html=True)
        
    # Initialiser l'√©tat de session
    initialize_session_state()

    # Afficher l'en-t√™te
    show_header()

    # Cr√©er la barre d'onglets horizontale pour la navigation
    pages = st.tabs([
        "üìä √âtat du r√©seau",
        "‚öôÔ∏è Configuration et d√©tection",
        "üìà R√©sultats",
        "üîß Insertion d'anomalies",
        "‚ÑπÔ∏è √Ä propos"
    ])

    # Onglet 0 : √âtat du r√©seau
    with pages[0]:
        show_network_status()

    # Onglet 1 : Configuration et d√©tection
    with pages[1]:
        show_detection_config()

    # Onglet 2 : R√©sultats
    with pages[2]:
        show_results()

    # Onglet 3 : Insertion d'anomalies
    with pages[3]:
        show_anomaly_insertion()

    # Onglet 4 : √Ä propos
    with pages[4]:
        show_about()

if __name__ == "__main__":
    main()